import stanza
import re

# T√©l√©charger le mod√®le fran√ßais si n√©cessaire
stanza.download("fr")

# Initialiser le pipeline NLP avec Tokenization, POS et NER
nlp = stanza.Pipeline("fr", processors="tokenize,pos,ner")

def clean_transcribed_text(text):
    """Nettoie le texte transcrit pour √©viter les erreurs NLP"""
    text = text.lower().strip()  # Mise en minuscules et suppression des espaces inutiles
    text = re.sub(r'\s+', ' ', text)  # Remplace plusieurs espaces par un seul
    text = re.sub(r'[^a-zA-Z√Ä-√ø0-9\s]', '', text)  # Supprime caract√®res sp√©ciaux sauf espaces
    text = re.sub(r'\b(euh|bah|heu|mmm|ben|ouais|voil√†)\b', '', text)  # Supprime les mots parasites
    return text.strip()

def check_sentence_completeness(text):
    """Ajoute un point si la phrase semble incompl√®te"""
    if not text.endswith(('.', '!', '?')):
        return text + '.'
    return text

def analyze_text(text):
    """Nettoie, v√©rifie et analyse un texte transcrit"""
    cleaned_text = clean_transcribed_text(text)
    checked_text = check_sentence_completeness(cleaned_text)

    print(f"\nüé§ Texte nettoy√© : {checked_text}")

    # Ex√©cuter le NLP
    doc = nlp(checked_text)

    print("Tokenization et Part-of-Speech Tagging :")
    for sentence in doc.sentences:
        for word in sentence.words:
            print(f"{word.text} -> {word.upos}")

    print("Reconnaissance d'Entit√©s Nomm√©es (NER) :")
    if not doc.ents:
        print("Aucune entit√© nomm√©e d√©tect√©e.")
    else:
        for sentence in doc.sentences:
            for ent in sentence.ents:
                print(f"Entit√© : {ent.text}, Type : {ent.type}")

# üîç Exemple de phrase transcrite (simulation du Speech-to-Text)
transcribed_text = "euh m√©t√©o Paris demain"
analyze_text(transcribed_text)
